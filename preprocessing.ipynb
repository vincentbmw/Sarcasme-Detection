{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vince\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from rapidfuzz import process, fuzz\n",
    "import nltk\n",
    "import multiprocessing as mp\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/processed_dataset/final_merged_dataset.csv')\n",
    "slang_words = pd.read_csv('dataset/additional/slang_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slang dictionary\n",
    "slang_dict = {str(k): str(v) for k, v in zip(slang_words['slang'], slang_words['meaning'])}\n",
    "\n",
    "# Load list kata KBBI (harus berupa list of words)\n",
    "with open('dataset/additional/list_kbbi.txt', 'r', encoding='utf-8') as f:\n",
    "    list_kbbi = [line.strip() for line in f.readlines() if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_repeated_chars(token):\n",
    "    \"\"\"Cek apakah token punya huruf berulang lebih dari 2x\"\"\"\n",
    "    return bool(re.search(r'(.)\\1{2,}', token))\n",
    "\n",
    "def correct_with_kbbi(token):\n",
    "    \"\"\"Koreksi token dengan RapidFuzz jika ada huruf berulang.\"\"\"\n",
    "    if has_repeated_chars(token):\n",
    "        # Kurangi huruf berulang dulu (biarkan max 2)\n",
    "        token = re.sub(r'(.)\\1{2,}', r'\\1\\1', token)\n",
    "        # Fuzzy match ke KBBI\n",
    "        match = process.extractOne(token, list_kbbi, scorer=fuzz.ratio)\n",
    "        if match and match[1] >= 70:\n",
    "            return match[0]\n",
    "    return token  # Tidak ada huruf berulang → langsung return\n",
    "\n",
    "def clean_and_normalize_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenisasi awal\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    normalized_tokens = []\n",
    "    for token in tokens:\n",
    "        # Slang normalization\n",
    "        if token in slang_dict:\n",
    "            normalized_tokens.append(slang_dict[token])\n",
    "        else:\n",
    "            # Hanya koreksi jika ada huruf berulang\n",
    "            corrected = correct_with_kbbi(token)\n",
    "            normalized_tokens.append(corrected)\n",
    "\n",
    "    # Gabungkan lagi\n",
    "    text = \" \".join(normalized_tokens)\n",
    "\n",
    "    # Hapus angka\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Pertahankan tanda baca penting\n",
    "    allowed_punct = \"!?.,…\"\n",
    "    text = ''.join(ch for ch in text if ch.isalnum() or ch.isspace() or ch in allowed_punct)\n",
    "\n",
    "    # Hapus spasi berlebih\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    # Tokenisasi ulang + stemming\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return \" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terapkan preprocessing\n",
    "df['tweet_processed'] = df['tweet'].fillna('').astype(str).apply(clean_and_normalize_text)\n",
    "df.to_csv('dataset/preprocessed_data_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
